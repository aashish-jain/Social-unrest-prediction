{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T10:45:50.762944Z",
     "start_time": "2019-04-19T10:45:49.760831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gensim\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "np = pd.np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-10T16:52:53.655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data for text processing\n",
    "\n",
    "#For removing punctuation\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#For Displaying progress\n",
    "tqdm_notebook(disable = True).pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T10:45:51.006831Z",
     "start_time": "2019-04-19T10:45:51.001035Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Reads articles from the given path.\n",
    "    path : path format from which files have to be read. e.g. ./*.cvs will read all csvs\n",
    "    Params:\n",
    "    show_progress : Shows the progress using tqdm if True else nothing displayed\n",
    "'''\n",
    "def read_articles(path, show_progress = True):\n",
    "    df_list = []\n",
    "    for file_name in tqdm_notebook(glob(path), disable = not show_progress):\n",
    "            temp_df = pd.read_csv(file_name, index_col=0)\n",
    "            temp_df[\"date\"] = file_name.split(\"/\")[-1].split('.')[0]\n",
    "            df_list.append(temp_df)\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    return df\n",
    "'''\n",
    "    For the given text, returns a list of words representing the text\n",
    "    with all words in lower case and punctuation along with stopwords \n",
    "    removed\n",
    "    Params:\n",
    "    text : Text for which vocubulary has to be generated\n",
    "'''\n",
    "def generate_document_vocabulary(text):\n",
    "    vocabulary = []\n",
    "    for word in word_tokenize(text):\n",
    "        w = word.translate(table).lower()\n",
    "        if w.isalpha() and w not in stop_words:\n",
    "            vocabulary.append(w)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the CSVs contain the following data\n",
    "date, title(headline), location, text(full article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing for doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T10:45:55.355466Z",
     "start_time": "2019-04-19T10:45:51.009997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read articles\n",
    "df = read_articles(\"../data/TOI/*.csv\")\n",
    "# Take training data (until 1-Jan-2019)\n",
    "df = df[df[\"date\"] < pd.to_datetime(\"1-Jan-2019\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T10:47:44.662588Z",
     "start_time": "2019-04-19T10:45:55.363673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the vocabulary from the given text\n",
    "df['vocabulary'] = df['text'].progress_apply(generate_document_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-10T17:00:54.325Z"
    }
   },
   "outputs": [],
   "source": [
    "#Convert the vocabulary into Tagged document for doc2vec model\n",
    "documents = []\n",
    "for i, row in df.iterrows():\n",
    "    document = TaggedDocument(row['vocabulary'], [i])\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T10:47:49.836760Z",
     "start_time": "2019-04-19T10:47:49.834100Z"
    }
   },
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 50\n",
    "alpha = 0.025\n",
    "\n",
    "# Distributed memory model\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha,\n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm=1,\n",
    "                workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T10:47:55.444881Z",
     "start_time": "2019-04-19T10:47:49.837942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T12:07:20.420875Z",
     "start_time": "2019-04-19T10:48:51.682698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for _ in tqdm_notebook(range(max_epochs)):\n",
    "    model.train(documents,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs,)\n",
    "    # Deacying learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T12:07:21.169781Z",
     "start_time": "2019-04-19T12:07:20.424029Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"article.d2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the document dictionary that can be used to access a document by tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the document dictionary that can be used to access a document by tag\n",
    "document_dic = {}\n",
    "for doc,tag in documents:\n",
    "    document_dic[tag[0]] = doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model for Random Data from ACLED after Jan-1-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"On July 15, a long protest march by farmers, from Mandsaur in Madhya Pradesh to New Delhi, demanding loan waiver and fair price for their produce, reached Jaipur.\"\n",
    "article = ' '.join(generate_document_vocabulary(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the closest Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb\n",
    "\n",
    "# find the vector\n",
    "vec = model.infer_vector(sabri)\n",
    "\n",
    "#find 10 closest documents\n",
    "sims = model.docvecs.most_similar([vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the document\n",
    "for doc_tag,score in sims:\n",
    "    print(\"Document has score : \"score, \"\\nContent : \" document_dic[doc_tag] + \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
