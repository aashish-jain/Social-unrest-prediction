{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T06:02:18.166992Z",
     "start_time": "2019-04-19T06:02:05.507865Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install tabulate\n",
    "!pip install vaderSentiment\n",
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T06:33:03.975182Z",
     "start_time": "2019-04-19T06:32:59.240819Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ateendraramesh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/ateendraramesh/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "import warnings\n",
    "import textblob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.utils as U\n",
    "import keras.layers as L\n",
    "import keras.models as M\n",
    "import keras.optimizers as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "from doc_utils import *\n",
    "from tabulate import tabulate\n",
    "from sklearn.ensemble import *\n",
    "from collections import Counter\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.linear_model import RidgeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nlp_utils import get_features, make_predictions\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T06:02:20.208317Z",
     "start_time": "2019-04-19T06:02:18.761289Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "\n",
    "# Read tweets csv\n",
    "df = pd.concat([pd.read_csv(\"../../data/Tweets3WeeksLocations.csv\"), pd.read_pickle(\"../../data/Tweets3Weeks_2Locations.pkl\")]) \n",
    "# Drop all-null rows if any\n",
    "df.dropna(how='all', inplace=True)\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T05:56:38.293922Z",
     "start_time": "2019-04-19T05:56:25.548Z"
    }
   },
   "outputs": [],
   "source": [
    "lead_days = 2\n",
    "days_window = 5\n",
    "\n",
    "start_date = pd.to_datetime(\"23-feb-2019\")\n",
    "end_date = pd.to_datetime(\"13-apr-2019\")\n",
    "\n",
    "\n",
    "labels = process_acled_csv(\"../../data/1900-01-01-2019-04-15-India.csv\", \n",
    "                           top_locations=-1, \n",
    "                           start=start_date, \n",
    "                           end=end_date,\n",
    "                           lead_days=lead_days,\n",
    "                           days_window = days_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T05:56:38.295365Z",
     "start_time": "2019-04-19T05:56:25.552Z"
    }
   },
   "outputs": [],
   "source": [
    "clean = False\n",
    "if not clean:\n",
    "    df = clean_df(df)\n",
    "    clean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T05:56:38.296382Z",
     "start_time": "2019-04-19T05:56:25.564Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T05:56:38.297983Z",
     "start_time": "2019-04-19T05:56:25.569Z"
    }
   },
   "outputs": [],
   "source": [
    "df = get_tweet_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T05:56:38.299460Z",
     "start_time": "2019-04-19T05:56:25.574Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_counter(df['lang'], num_elements=10, xlabel=\"Language\", ylabel=\"Number of Tweets\", title=\"Language Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T05:56:38.300589Z",
     "start_time": "2019-04-19T05:56:25.576Z"
    }
   },
   "outputs": [],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "# Sort by time created\n",
    "df.sort_values(by=['created_at'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T05:56:38.301901Z",
     "start_time": "2019-04-19T05:56:25.581Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(start_date, end_date)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "location_date_dict = interleave_location_and_date(df, start_date, end_date)\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Features\n",
    "   * Number of tweets each day\n",
    "   * Average pos, neg, neu and compound features\n",
    "   * Tweet count with neg sentiment\n",
    "\n",
    "### TODO\n",
    "   * Hate speech\n",
    "   * Violent speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T05:56:38.303223Z",
     "start_time": "2019-04-19T05:56:25.585Z"
    }
   },
   "outputs": [],
   "source": [
    "location_features_dict = {}\n",
    "for location in location_date_dict:\n",
    "    location_features_dict[location] = get_features(location_date_dict[location])\n",
    "    print(location, \"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T05:56:38.304679Z",
     "start_time": "2019-04-19T05:56:25.588Z"
    }
   },
   "outputs": [],
   "source": [
    "make_predictions(location_features_dict, labels, permute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T06:35:56.839538Z",
     "start_time": "2019-04-19T06:35:56.821685Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_LSTM_model(history):\n",
    "    \"\"\" Generates a compiled LSTM model\n",
    "    \n",
    "    Input - Number of history points considered\n",
    "    \"\"\"\n",
    "    model = M.Sequential()\n",
    "    model.add(L.InputLayer(input_shape=(history, 6)))\n",
    "    model.add(L.LSTM(10))\n",
    "    model.add(L.Dense(20, activation='relu'))\n",
    "    model.add(L.Dense(2))\n",
    "    model.add(L.Softmax())\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt.Adam(0.01))\n",
    "    return model\n",
    "\n",
    "def generate_CNN_model(history):\n",
    "    \"\"\" Generate a compiled 1D CNN model\n",
    "    \n",
    "    Input - Number of history points considered\n",
    "    \"\"\"\n",
    "    model = M.Sequential()\n",
    "    model.add(L.InputLayer(input_shape=(history, 6)))\n",
    "    model.add(L.Conv1D(32, kernel_size=3))\n",
    "    model.add(L.Conv1D(16, kernel_size=1))\n",
    "    model.add(L.Flatten())\n",
    "    model.add(L.Dense(20, activation='relu'))\n",
    "    model.add(L.Dense(2))\n",
    "    model.add(L.Softmax())\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt.Adam(0.01))\n",
    "    return model\n",
    "\n",
    "def make_deep_predictions(location_features_dict, labels, model=None, permute=False, lead_days=2, days_window=5, history=3):\n",
    "    \"\"\"\n",
    "    Input - \n",
    "            location_features_dict - The dict mapping from location to features\n",
    "            labels - Label dict generated from process_acled_csv(..)\n",
    "            model - Specific sklearn model to evaluate/benchmark performance\n",
    "            permute - Permute the data before train-test split\n",
    "            history - The number of data points for contextualization\n",
    "    Returns - None\n",
    "    \"\"\"\n",
    "    # Table for presenting on tabulate\n",
    "    result_table = []\n",
    "\n",
    "    # Compute intersection for locations present on both dicts\n",
    "    common_locations = set(location_features_dict.keys()) & set(labels.keys())\n",
    "\n",
    "    # Sorted for clarity\n",
    "    common_locations = sorted(list(common_locations))\n",
    "\n",
    "    for common_location in common_locations:\n",
    "        # Get data and labels\n",
    "        X, y = location_features_dict[common_location], labels[common_location]\n",
    "        X, y = np.array(X), np.array(y)\n",
    "\n",
    "        # Eliminate last days to match labels.shape\n",
    "        X = X[:-(lead_days + days_window)]\n",
    "        \n",
    "        # Generate data for LSTM/CNN\n",
    "        # Basically, use points from i to i + history\n",
    "        # and predict for i + history + 1\n",
    "        temp_X, temp_y = [], []\n",
    "        for i in range(len(X) - history - 1):\n",
    "            temp_X.append(X[i: i + history])\n",
    "            temp_y.append(y[i + history + 1])\n",
    "        \n",
    "        X, y = np.array(temp_X), np.array(temp_y)\n",
    "        \n",
    "        # Permute randomly if specified\n",
    "        if permute:\n",
    "            p = np.random.permutation(len(X))\n",
    "            X, y = X[p], y[p]\n",
    "\n",
    "        # Split data into train & test - 75% & 25%\n",
    "        split = int(0.75 * len(X))\n",
    "        \n",
    "        xtrain, ytrain = X[:split], y[:split]\n",
    "        xtest, ytest = X[split:], y[split:]\n",
    "        \n",
    "        \n",
    "        model = generate_LSTM_model(history)\n",
    "\n",
    "\n",
    "        ytrain, ytest = U.to_categorical(ytrain, num_classes=2), U.to_categorical(ytest, num_classes=2)\n",
    "        # Fit the train data\n",
    "        model.fit(xtrain, ytrain, epochs=100, verbose=0)\n",
    "\n",
    "        # Make predictions\n",
    "        ypred = model.predict(xtest)\n",
    "        ytrain_pred = model.predict(xtrain)\n",
    "\n",
    "        # Uncategorize\n",
    "        uncategorize = lambda x: np.argmax(x, axis=1)\n",
    "\n",
    "        ytrain, ytest = uncategorize(ytrain), uncategorize(ytest)\n",
    "        ytrain_pred, ypred = uncategorize(ytrain_pred), uncategorize(ypred)\n",
    "\n",
    "        # Compute metrics\n",
    "        train_acc = np.mean(ytrain_pred == ytrain)\n",
    "        test_acc = np.mean(ytest == ypred)\n",
    "        precision = precision_score(ytest, ypred, average='weighted', labels=np.unique(ypred))\n",
    "        recall = recall_score(ytest, ypred, average='weighted', labels=np.unique(ypred))\n",
    "        f1 = f1_score(ytest, ypred, average='weighted', labels=np.unique(ypred))\n",
    "\n",
    "        # Add row to result_table\n",
    "        result_row = [common_location,\n",
    "                      np.round(train_acc, 2), np.round(test_acc, 2),\n",
    "                      np.round(precision, 2), np.round(recall, 2),\n",
    "                      np.round(f1, 2),\n",
    "                      np.round(np.sum(y) / len(y), 2)]\n",
    "        result_table.append(result_row)\n",
    "\n",
    "    # Average stats\n",
    "    # Turns out median is kind of useless\n",
    "    result_table_copy = (np.array(result_table)[:, 1:]).astype(np.float32)\n",
    "    result_table = sorted(result_table, key=lambda x: -x[2])\n",
    "    averages = np.round(np.mean(result_table_copy, axis=0), 2)\n",
    "\n",
    "    # Add them to the existing result table\n",
    "    result_table.append([\"Average\"] + averages.tolist())\n",
    "\n",
    "    # Header for table\n",
    "    header = [\"Location\", \"Train Accuracy\", \"Test Accuracy\",\n",
    "              \"Precision\", \"Recall\", \"F1 Score\", \"+'s in data\"]\n",
    "    \n",
    "    # Print tabulated result\n",
    "    print(tabulate(result_table, \n",
    "                   tablefmt=\"pipe\", \n",
    "                   stralign=\"center\", \n",
    "                   headers=header))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T05:56:38.307106Z",
     "start_time": "2019-04-19T05:56:25.595Z"
    }
   },
   "outputs": [],
   "source": [
    "make_deep_predictions(location_features_dict, labels, permute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
