{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T06:02:50.362195Z",
     "start_time": "2019-04-19T06:02:50.357772Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install twython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T06:02:50.852875Z",
     "start_time": "2019-04-19T06:02:50.373993Z"
    }
   },
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time\n",
    "import yaml\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up  twython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T06:02:51.377117Z",
     "start_time": "2019-04-19T06:02:50.953545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up twython\n",
    "\n",
    "APP_KEY = '9vXK2iKIqDjwkI70ImVyUySyN'\n",
    "APP_SECRET = \"3qdL7Jp5MMXcjy96CBh0SWoAqmQxUZysanfscAMYxuJrb4YGe8\"\n",
    "\n",
    "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locations and Keywords used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T21:51:04.669226Z",
     "start_time": "2019-04-14T21:51:04.616511Z"
    }
   },
   "outputs": [],
   "source": [
    "# Locations used to query tweets\n",
    "\n",
    "locationsTemp=[\"chennai\",\"delhi\",\"india\",\"ahmedabad\",\"gujarat\",\"tamil\",\"jammu\",\"srinagar\",\"imphal\",\"lucknow\",\n",
    "                \"bengaluru\",\"bangalore\",\"bathinda\",\"chandigarh\",\"ludhiana\",\"amritsar\",\"punjab\",\"arunachal\",\"assam\",\n",
    "                \"kolkata\",\"westbengal\",\"kerala\",\"karnataka\",\"andhra\",\"telangana\",\"hyderabad\",\"madhya\",\"uttar\",\n",
    "                \"maharashtra\",\"haryana\",'Jammu','Srinagar','Delhi-NewDelhi','Bathinda','Dehradun','Chandigarh',\n",
    "                'Ludhiana','Amritsar','Imphal','Lucknow','Jalandhar','Kolkata','Guwahati','Chennai','Patiala',\n",
    "                'Bhubaneswar','Bengaluru','Patna','Jaipur','Coimbatore','Hyderabad','Ranchi','Thiruvananthapuram',\n",
    "                'Shimla','Sangrur','Ahmedabad','Karnal','Pulwama','Puducherry','Gurgaon','Agartala','Madurai',\n",
    "                'Tiruchirappalli','Bangkok','Salem']\n",
    "\n",
    "locations = set()\n",
    "for location in locationsTemp:\n",
    "    locations.add(location.lower())\n",
    "locations = pd.DataFrame(list(locations))\n",
    "locations.columns = [\"Locations\"]\n",
    "locations.to_csv(\"Locations of Interest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T21:51:06.268607Z",
     "start_time": "2019-04-14T21:51:06.262178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keywords used to query tweets\n",
    "\n",
    "keywords = ['protest','protests','riot','riots','violence','unrest','clash','bandh','issue','rally','demonstration','election','polls','attac']\n",
    "keywords = pd.DataFrame(keywords)\n",
    "keywords.columns = [\"Keywords\"]\n",
    "keywords.to_csv(\"Keywords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T21:51:06.983114Z",
     "start_time": "2019-04-14T21:51:06.972203Z"
    }
   },
   "outputs": [],
   "source": [
    "locations = pd.read_csv(\"Locations of Interest.csv\", index_col=0)\n",
    "keywords = pd.read_csv(\"Keywords.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T21:51:09.052260Z",
     "start_time": "2019-04-14T21:51:09.048068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting the keywords and locations to format suitable for Twitter API\n",
    "\n",
    "keywordsQuery = ''.join(map(str, [keyword+\" OR \" for keyword in list(keywords['Keywords'])]))[:-4]\n",
    "start = 0\n",
    "to_return = []\n",
    "loc_list =locations[\"Locations\"].values.tolist()\n",
    "len_loc = len(loc_list) // 5 + 1\n",
    "for i in range(5):\n",
    "    to_return.append(loc_list[start:start+len_loc])\n",
    "    start+=len_loc\n",
    "    \n",
    "locationQueries=[]\n",
    "for row in to_return:\n",
    "    locationQueries.append(''.join(map(str, [location+\" OR \" for location in row]))[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locations Queries\n",
      "\n",
      " ['karnataka OR madhya OR tiruchirappalli OR karnal OR jaipur OR jalandhar OR bangalore OR chandigarh OR agartala OR ludhiana OR imphal', 'delhi-newdelhi OR salem OR patna OR andhra OR hyderabad OR gurgaon OR sangrur OR madurai OR haryana OR ranchi OR shimla', 'bangkok OR westbengal OR kolkata OR lucknow OR india OR bhubaneswar OR tamil OR coimbatore OR kerala OR telangana OR bengaluru', 'patiala OR thiruvananthapuram OR amritsar OR assam OR arunachal OR maharashtra OR ahmedabad OR srinagar OR delhi OR punjab OR puducherry', 'guwahati OR pulwama OR dehradun OR chennai OR gujarat OR jammu OR uttar OR bathinda'] \n",
      "\n",
      "Keywords Query\n",
      "\n",
      " protest OR protests OR riot OR riots OR violence OR unrest OR clash OR bandh OR issue OR rally OR demonstration OR election OR polls OR attac\n"
     ]
    }
   ],
   "source": [
    "print(\"Locations Queries\\n\\n\", locationQueries,\"\\n\")\n",
    "print(\"Keywords Query\\n\\n\", keywordsQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: generator 'Twython.cursor' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10000\n",
      "Progress: 20000\n",
      "Progress: 30000\n",
      "Progress: 40000\n",
      "Thread sleep\n"
     ]
    }
   ],
   "source": [
    "# Querying twitter API\n",
    "\n",
    "date = dt.datetime.now()\n",
    "cnt = 0\n",
    "tweetList = []\n",
    "for locationQuery in locationQueries:\n",
    "    tweets = twitter.cursor(twitter.search, q=keywordsQuery + ' (' +locationQuery + ') -filter:retweets', count=100, tweet_mode = \"extended\")\n",
    "    for item in tweets:\n",
    "        if (cnt%10000 == 0):\n",
    "            print(\"Progress: \"+str(cnt))\n",
    "        tweetList.append(item)\n",
    "        cnt+=1\n",
    "        if cnt > 40000:\n",
    "            print(\"Thread sleep\")\n",
    "            time.sleep(60*15+5)\n",
    "            cnt = 0\n",
    "tweetsDF = pd.DataFrame(tweetList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup to get full text for each tweet (first fetch gets only truncated text) \n",
    "\n",
    "cnt = 1\n",
    "rowInd = 1\n",
    "tempList = []\n",
    "tweetList = []\n",
    "tweetsDfs = []\n",
    "tweets = tweetsDF\n",
    "for index, row in tweets.iterrows():\n",
    "    tempList.append(row[\"id\"])\n",
    "    if cnt%100 == 0:\n",
    "        try:\n",
    "            tweetList.extend(twitter.lookup_status(id=tempList, tweet_mode = \"extended\"))\n",
    "            tempList = []\n",
    "        except:\n",
    "            print(\"Succesful till: \"+cnt)\n",
    "    if (cnt%10000 == 0):\n",
    "        print(\"Progress: \"+str(cnt))\n",
    "        if cnt >= 40000:\n",
    "            tweetsDfs.append(pd.DataFrame(tweetList))\n",
    "#             tweetsDf.to_csv(\"Tweets3Weeks\"+str(date.day)+str(date.strftime(\"%m\"))+\"_\"+str(rowInd)+\".csv\")\n",
    "            tweetList = []\n",
    "            rowInd+=1\n",
    "#             print(\"Thread sleep\")\n",
    "#             time.sleep(60*15+5)\n",
    "            cnt = 0\n",
    "    cnt+=1\n",
    "tweetList.extend(twitter.lookup_status(id=tempList, tweet_mode = \"extended\"))\n",
    "tweetsDfs.append(pd.DataFrame(tweetList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T21:54:24.230646Z",
     "start_time": "2019-04-14T21:54:18.259984Z"
    }
   },
   "outputs": [],
   "source": [
    "tweetsDF = pd.concat(tweetsDfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF.shape\n",
    "tweetsDF.index = np.arange(len(tweetsDF))\n",
    "date = dt.datetime.now()\n",
    "tweetsDF.to_csv(\"tweets_collected_\"+str(date.day)+str(date.strftime(\"%m\"))+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing tweets to get useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T22:40:37.210288Z",
     "start_time": "2019-04-14T22:40:37.202088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get user related features\n",
    "\n",
    "def get_features(row):\n",
    "    row[\"userlocation\"] = row[\"user\"][\"location\"]\n",
    "    row[\"username\"] = row[\"user\"][\"name\"]\n",
    "    row[\"userscreen_name\"] = row[\"user\"][\"screen_name\"]\n",
    "    row[\"userdescription\"] = row[\"user\"][\"description\"]\n",
    "    row[\"userfollowers_count\"] = row[\"user\"][\"followers_count\"]\n",
    "    row[\"userfriends_count\"] = row[\"user\"][\"friends_count\"]\n",
    "    row[\"userlisted_count\"] = row[\"user\"][\"listed_count\"]\n",
    "    row[\"userfavourites_count\"] = row[\"user\"][\"favourites_count\"]\n",
    "    row[\"userverified\"] = row[\"user\"][\"verified\"]\n",
    "    row[\"userstatuses_count\"] = row[\"user\"][\"statuses_count\"]\n",
    "    row[\"userfollowing\"] = row[\"user\"][\"following\"]\n",
    "    row[\"userfollow_request_sent\"] = row[\"user\"][\"follow_request_sent\"]\n",
    "    row[\"usercontributors_enabled\"] = row[\"user\"][\"contributors_enabled\"]\n",
    "    sepr =  \"\" \n",
    "    l = []\n",
    "    for location in locations['Locations']: \n",
    "        if (location in str(row[\"full_text\"]).lower()):\n",
    "            l.append(location)\n",
    "    row[\"extracted_location\"] = ','.join(l)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-14T23:29:58.927542Z",
     "start_time": "2019-04-14T22:41:45.855742Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_data = tweetsDF.apply(get_features, axis=1)\n",
    "processed_data.index = np.arange(len(processed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T06:09:45.271947Z",
     "start_time": "2019-04-19T06:09:45.245655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map tweets to acled locations\n",
    "\n",
    "acledLocations = ['Jammu','Srinagar','Delhi-New Delhi','Bathinda','Dehradun','Chandigarh','Ludhiana','Amritsar',\n",
    "                  'Imphal','Lucknow','Jalandhar','Kolkata','Guwahati','Chennai','Patiala','Bhubaneswar','Bengaluru',\n",
    "                  'Patna','Jaipur','Coimbatore','Hyderabad','Ranchi','Thiruvananthapuram','Shimla','Sangrur',\n",
    "                  'Ahmedabad','Karnal','Pulwama','Puducherry','Gurgaon','Agartala','Madurai','Tiruchirappalli',\n",
    "                  'Bangkok','Salem','India']\n",
    "\n",
    "def generate_locations(row):\n",
    "    dfrow_list = []\n",
    "    for location in acledLocations: \n",
    "        tempLocation = location\n",
    "        tempLocation2 = location\n",
    "        if (tempLocation == \"Delhi-New Delhi\"):\n",
    "            tempLocation = \"delhi\"\n",
    "        elif (tempLocation == \"Bengaluru\"):\n",
    "            tempLocation = \"bangalore\"\n",
    "            tempLocation2 = \"karnataka\"\n",
    "        elif (tempLocation == \"Kolkata\"):\n",
    "            tempLocation = \"west bengal\"\n",
    "        elif (tempLocation == \"Guwahati\"):\n",
    "            tempLocation = \"assam\"\n",
    "        elif (tempLocation in (\"Lucknow\", \"Dehradun\")):\n",
    "            tempLocation = \"uttar\"\n",
    "        elif (tempLocation in (\"Sangrur\", \"Ludhiana\", \"Amritsar\", \"Chandigarh\", \"Bathinda\", \"Jalandhar\", \"Patiala\")):\n",
    "            tempLocation = \"punjab\"\n",
    "        elif (tempLocation in (\"Karnal\", \"Gurgaon\", \"Chandigarh\")):\n",
    "            tempLocation2 = \"haryana\"\n",
    "        elif (tempLocation == \"Pulwama\"):\n",
    "            tempLocation2 = \"jammu\"\n",
    "        elif (tempLocation == \"Ahmedabad\"):\n",
    "            tempLocation = \"gujarat\"\n",
    "        elif (tempLocation in (\"Chennai\", \"Coimbatore\", \"Madurai\", \"Tiruchirappalli\", \"Salem\")):\n",
    "            tempLocation = \"tamil\"\n",
    "        elif (tempLocation in (\"Imphal\", \"Agartala\")):\n",
    "            tempLocation = \"arunachal\"\n",
    "        elif (tempLocation == \"Hyderabad\"):\n",
    "            tempLocation = \"andhra\"\n",
    "            tempLocation2 = \"telangana\"\n",
    "        elif (tempLocation == \"Thiruvananthapuram\"):\n",
    "            tempLocation = \"kerala\"\n",
    "        if (tempLocation.lower() in row[\"extracted_location\"] or location.lower() in row[\"extracted_location\"] or tempLocation2 in row[\"extracted_location\"]):\n",
    "            row[\"finallocation\"] = location\n",
    "            dfrow_list.append(row.copy())\n",
    "    return pd.DataFrame(dfrow_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "posssed_df = pd.concat(processed_data.apply(generate_locations, axis = 1).tolist())\n",
    "posssed_df.index = np.arange(len(posssed_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_json(\"../../../data/TweetsConsolidted.json\")\n",
    "    df = pd.concat([posssed_df, df])\n",
    "    df.index = np.arange(len(df))\n",
    "except:\n",
    "    df = posssed_df\n",
    "df.to_json(\"../../../data/TweetsConsolidated.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
